{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RandomForrest_kPCA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMaJc7qEsowrB1/96znwxaC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nietzscholas9000/UsedCarsProject/blob/master/RandomForrest_kPCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zY4_FpKHAEPA",
        "colab": {}
      },
      "source": [
        "Ny_components = 4000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BI8AyEkCEJ7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Approximate Kernel PCA using Nystroem\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "#Loading the dataset\n",
        "try:\n",
        "  dataset = pd.read_csv('drive/My Drive/processed.csv')\n",
        "except:\n",
        "  dataset = pd.read_csv('processed.csv')\n",
        "\n",
        "X = dataset.iloc[:,2:].values\n",
        "y = dataset.iloc[:,1].values\n",
        "\n",
        "#Splitting the data into Training Set and Test Set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaS2kzmQEL-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import required libraries for approximate Kernel PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.kernel_approximation import Nystroem\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#Normalizing the features\n",
        "sc_X = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)\n",
        "\n",
        "# Nystroem approximation on a subset of data\n",
        "# (used because kernelPCA is not feasible on the large dataset)\n",
        "Ny = Nystroem(n_components=Ny_components, random_state=0)\n",
        "X_train = Ny.fit_transform(X_train)\n",
        "X_test = Ny.transform(X_test)\n",
        "\n",
        "#Linear PCA\n",
        "PCAObj = PCA(n_components=None, random_state=0)\n",
        "X_train = PCAObj.fit_transform(X_train)\n",
        "X_test = PCAObj.transform(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSZ6J-WlET7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define cumulative explained variance ratio\n",
        "total_variance = np.cumsum(PCAObj.explained_variance_ratio_)\n",
        "\n",
        "# Show when total_variance is 0.95 using a plot\n",
        "plt.figure(1, figsize=[16,12])\n",
        "plot1 = plt.subplot(211)\n",
        "plt.plot(PCAObj.explained_variance_ratio_, c='red')\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "\n",
        "plot2 = plt.subplot(212)\n",
        "plt.plot(total_variance, c='blue')\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Total explained variance')\n",
        "# Look for where cumulative explained variance is 0.95\n",
        "expl_95_index = np.where((total_variance > 0.949) & (total_variance < 0.951))[0][0]\n",
        "print('Number of components with 0.95 cumulative variance: ', expl_95_index)\n",
        "plt.hlines(0.95, 0, len(total_variance), linestyles='dashed', color='green')\n",
        "plt.vlines(expl_95_index, 0, 1, linestyles='dashed')\n",
        "plot1.vlines(expl_95_index, 0, np.max(PCAObj.explained_variance_ratio_), linestyles='dashed')\n",
        "plt.text(0, 0.95-0.05, 'y = 0.95', color='green')\n",
        "plt.text(expl_95_index+2, 0, 'x = '+str(expl_95_index))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlxuFeSUEU9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reset_selective -f ^(?!expl_95_index|Ny_components).*$ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0E0YGuYyd7EW",
        "colab": {}
      },
      "source": [
        "#Kernal PCA (Nystroem approximation)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "#Loading the dataset\n",
        "try:\n",
        "  dataset = pd.read_csv('drive/My Drive/processed.csv')\n",
        "except:\n",
        "  dataset = pd.read_csv('processed.csv')\n",
        "\n",
        "\n",
        "X = dataset.iloc[:,2:].values\n",
        "y = dataset.iloc[:,1].values\n",
        "\n",
        "#Splitting the data into Training Set and Test Set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gteKNzfwEo56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import required libraries for making a PCA pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Pipeline for predicting the target\n",
        "RF_PCA_pipeline = Pipeline([\n",
        "                    ('scale', StandardScaler()), \n",
        "                    ('PCA', PCA(n_components=expl_95_index, random_state=0)),\n",
        "                    ('RF', RandomForestRegressor(random_state=0, warm_start=True))\n",
        "                     ], verbose=True)\n",
        "grid_param = dict(rf__n_estimators=[10, 30, 50],\n",
        "                  rf__min_impurity_decrease=[0.0001, 0.0005, 0.001, 0.005, 0.01],\n",
        "                  rf__bootstrap=[True,False])\n",
        "# Using a grid search to test parameters\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "searchObj = GridSearchCV(RF_PCA_pipeline, grid_param, cv=10, scoring='neg_mean_squared_error', n_jobs=-1, verbose=10)\n",
        "searchObj.fit(X_train, y_train)\n",
        "print(searchObj.best_params_)  \n",
        "print(searchObj.best_score_)\n",
        "print(searchObj.cv_results_)\n",
        "\n",
        "RF_PCA_pipeline.fit(X_train, y_train)\n",
        "y_pred = RF_PCA_pipeline.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbTaMdd7E6NE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluating the model\n",
        "print(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print(y_test)\n",
        "print(y_pred)\n",
        "print(mean_squared_error(10**y_test, 10**y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiTEFZTmE_K9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_param = dict(rf__n_estimators=[10, 30, 50],\n",
        "                  rf__min_impurity_decrease=[0.0001, 0.0005, 0.001, 0.005, 0.01],\n",
        "                  rf__bootstrap=[True,False])\n",
        "plt.plot(grid_param['rf__min_impurity_decrease'], mean_scores[0,:,0],\n",
        "         grid_param['rf__min_impurity_decrease'], mean_scores[0,:,1],\n",
        "         grid_param['rf__min_impurity_decrease'], mean_scores[0,:,2]\n",
        "         )\n",
        "plt.ylabel('validation MSE (log-scale)')\n",
        "plt.xlabel('minimum impurity decrease of Random Forest')\n",
        "plt.title('Validation Scores for Boostrap=True')\n",
        "plt.legend(grid_param['rf__n_estimators'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}